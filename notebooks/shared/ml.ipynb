{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "sys.path.append(\"/Users/nwong/Workspace/Projects/tdt4173_project/src\")\n",
    "\n",
    "from feature_engineering.sklearn_transformers import *\n",
    "from feature_engineering.store_features import *\n",
    "from feature_engineering.bus_stop_features import *\n",
    "from feature_engineering.utils import *\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "stop_importance_levels = ['Mangler viktighetsniv√•',\n",
    "                          'Standard holdeplass',\n",
    "                          'Lokalt knutepunkt',\n",
    "                          'Nasjonalt knutepunkt',\n",
    "                          'Regionalt knutepunkt',\n",
    "                          'Annen viktig holdeplass']\n",
    "store_types = ['lv1_desc', 'lv2_desc', 'lv3_desc', 'lv4_desc']\n",
    "geo_groups = ['grunnkrets_id','t_district','municipality_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nwong/Workspace/Projects/tdt4173_project/src/feature_engineering/bus_stop_features.py:11: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  bus_stops_df[['lon', 'lat']] = bus_stops_df['lng_lat'].str.split(\n"
     ]
    }
   ],
   "source": [
    "stores_train_df = set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/stores_train.csv\"))\n",
    "stores_test_df = set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/stores_test.csv\"))\n",
    "stores_extra_df = set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/stores_extra.csv\"))\n",
    "\n",
    "income_dist = set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/grunnkrets_income_households.csv\"))\n",
    "age_dist = set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/grunnkrets_age_distribution.csv\"))\n",
    "household_dist = set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/grunnkrets_households_num_persons.csv\"))\n",
    "grunnkrets_df = combine_keys(set_year_2016(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/grunnkrets_norway_stripped.csv\")))\n",
    "plaace_df = pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/plaace_hierarchy.csv\")\n",
    "\n",
    "bus_stops_df = bus_stops_lat_lon(pd.read_csv(\"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/busstops_norway.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train_merged = stores_train_df.merge(plaace_df, on=\"plaace_hierarchy_id\", how=\"left\").drop(['revenue'], axis=1)\n",
    "stores_test_merged = stores_test_df.merge(plaace_df, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "stores_extra_merged = stores_extra_df.merge(plaace_df, on=\"plaace_hierarchy_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pop_density(stores_df, age_dist, grunnkrets_df, geo_groups):\n",
    "    population_density = population_density_grouped_by_geo_group(stores_df, age_dist, grunnkrets_df, geo_groups)\n",
    "    return population_density.fillna(population_density.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def new_age_dist(stores_df, age_df, grunnkrets_df, geo_groups):\n",
    "    _age_dist = age_dist_by_geo_group(stores_train_df, age_dist, grunnkrets_df, geo_groups)\n",
    "    return _age_dist.fillna(_age_dist.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_simil = ClusterSimilarity(n_clusters=100, gamma=1., random_state=42, sample_weight=stores_train_df[['revenue']])\n",
    "similarities = cluster_simil.fit(stores_train_df[[\"lat\", \"lon\"]])\n",
    "\n",
    "def new_clustering(cluster_simil, stores_df):\n",
    "    return pd.DataFrame(cluster_simil.transform(stores_df[['lat', 'lon']]), columns=cluster_simil.get_feature_names_out(), index=stores_df.store_id).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stores_train = stores_train_df \\\n",
    "    .merge(income_dist, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(grunnkrets_df, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(bus_stops_distance_by_importance(stores_train_df, bus_stops_df, stop_importance_levels).reset_index(level=0), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(store_closest_by_store_groups(stores_train_merged, pd.concat([stores_train_merged, stores_test_merged, stores_extra_merged]), store_types), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_pop_density(stores_train_df, age_dist, grunnkrets_df, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_age_dist(stores_train_df, age_dist, grunnkrets_df, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    # .merge(new_clustering(cluster_simil, stores_train_df), on=\"store_id\", how=\"left\")\n",
    "merged_stores_test = stores_test_df \\\n",
    "    .merge(grunnkrets_df, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(income_dist, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(bus_stops_distance_by_importance(stores_test_df, bus_stops_df, stop_importance_levels).reset_index(level=0), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(store_closest_by_store_groups(stores_test_merged, pd.concat([stores_train_merged, stores_test_merged, stores_extra_merged]), store_types), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_pop_density(stores_test_df, age_dist, grunnkrets_df, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_age_dist(stores_test_df, age_dist, grunnkrets_df, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    # .merge(new_clustering(cluster_simil, stores_test_df), on=\"store_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = merged_stores_train[['store_id', 'revenue']].copy()\n",
    "merged_stores_train = merged_stores_train.copy().drop('revenue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_transformer(merged_stores_df):\n",
    "    yeo_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        PowerTransformer()\n",
    "    )\n",
    "    preprocessing = make_column_transformer(\n",
    "        (yeo_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        remainder = \"passthrough\"\n",
    "    )\n",
    "    return pd.DataFrame(preprocessing.fit_transform(merged_stores_df), columns=preprocessing.get_feature_names_out(), index=merged_stores_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stores_train = new_transformer(merged_stores_train)\n",
    "merged_stores_test = new_transformer(merged_stores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer()\n",
    "rev_transformed = pt.fit_transform(target_labels[[\"revenue\"]])\n",
    "merged_stores_train[\"revenue\"] = rev_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['remainder__geometry', 'remainder__grunnkrets_name', 'remainder__district_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stores_train.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stores_test.drop(drop_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the H2O cluster (locally)\n",
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "train = h2o.H2OFrame(merged_stores_train)\n",
    "test = h2o.H2OFrame(merged_stores_test)\n",
    "#test = h2o.H2OFrame(pd.concat([test_set, stores_test_enriched[['store_id']]], axis=1).drop(drop_cols, axis=1))\n",
    "\n",
    "# Identify predictors and response\n",
    "x = train.columns\n",
    "y = \"revenue\"\n",
    "x.remove(y)\n",
    "\n",
    "# Run AutoML for 20 base models\n",
    "aml = H2OAutoML(max_models=20, seed=1, exclude_algos=['deeplearning'])\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The leader model is stored here\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = h2o.get_model(lb[3,\"model_id\"])\n",
    "m.varimp(use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_avg = aml.predict(test)\n",
    "preds_best = aml.leader.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test.cbind(preds_best)\n",
    "df = df.as_data_frame(use_pandas=True)\n",
    "result = df.loc[:,(\"store_id\", 'predict')]\n",
    "submission = result.rename(columns = {\"store_id\": \"id\",  \"predict\" : \"predicted\"})\n",
    "submission['predicted'] = pt.inverse_transform(submission[['predicted']])\n",
    "submission.to_csv(\"StackedEnsembleBestOfFamily4.csv\", index = False)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sklearn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b486652d2e6c5ac00f1af9aaa5d14fac25fa6ee0068e5fd7a5ee238732fb741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
