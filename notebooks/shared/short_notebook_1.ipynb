{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_read(path, file_name, dtype=None):\n",
    "    \"\"\"\n",
    "        Utility function to simplify reading of files from local machine.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"{path}{file_name}.csv\", dtype=dtype)\n",
    "\n",
    "def deduplicate_year(raw_df, deduplicate_column=\"grunnkrets_id\"):\n",
    "    \"\"\"\n",
    "        Use 2016 values by default. If exist in 2015, merge together. Drop year.\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.copy()\n",
    "    return raw_df.sort_values(by='year').drop_duplicates(subset=[deduplicate_column], keep='last').drop('year', axis=1)\n",
    "\n",
    "def combine_keys(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe['t_district'] = dataframe['district_name'] + dataframe['municipality_name']\n",
    "    return dataframe\n",
    "\n",
    "def bus_stops_lat_lon(bus_stops_df):\n",
    "    \"\"\"\n",
    "    Extract latitude and longitude as separate columns.\n",
    "    \"\"\"\n",
    "    bus_stops_df['lng_lat'] = bus_stops_df['geometry'].str.extract(\n",
    "        r'\\((.*?)\\)')\n",
    "    bus_stops_df[['lon', 'lat']] = bus_stops_df['lng_lat'].str.split(\n",
    "        \" \", 1, expand=True)\n",
    "    bus_stops_df[['lon', 'lat']] = bus_stops_df[[\n",
    "        'lon', 'lat']].apply(pd.to_numeric)\n",
    "    return bus_stops_df[['busstop_id', 'stopplace_type', 'importance_level', 'side_placement', 'geometry', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bus_stops_closest(stores_df, bus_stops_df, importance_level=\"Regionalt knutepunkt\"):\n",
    "    \"\"\"\n",
    "    Id and distance of the closest bus stop to all stores.\n",
    "    \"\"\"\n",
    "    bus_stops_df = bus_stops_df[bus_stops_df['importance_level'] == importance_level]\n",
    "    mat = cdist(stores_df[['lat', 'lon']],\n",
    "                bus_stops_df[['lat', 'lon']], metric='euclidean')\n",
    "\n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=bus_stops_df['busstop_id'])\n",
    "\n",
    "    stores = stores_df.store_id\n",
    "    closest = new_df.idxmin(axis=1)\n",
    "    distance = new_df.min(axis=1)\n",
    "\n",
    "    return pd.DataFrame({'store_id': stores.values, 'closest_bus_stop': closest.values, 'distance': distance.values})\n",
    "\n",
    "def bus_stops_in_radius(stores_df, bus_stops_df, radius=0.1, importance_level=None):\n",
    "    \"\"\"\n",
    "    Number of bus stops within a given radius. The importance level of bus stops can be specified.\n",
    "    \"\"\"\n",
    "    if importance_level is not None:\n",
    "        bus_stops_df = bus_stops_df[bus_stops_df['importance_level'] == importance_level]\n",
    "\n",
    "    mat = cdist(stores_df[['lat', 'lon']],\n",
    "                bus_stops_df[['lat', 'lon']], metric='euclidean')\n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=bus_stops_df['busstop_id'])\n",
    "    count = pd.DataFrame(new_df[new_df < radius].count(axis=1)).reset_index()\n",
    "    count.rename(columns={0: 'count'}, inplace=True)\n",
    "    return count\n",
    "\n",
    "# Relevant feature engineering functions.\n",
    "def bus_stops_distance_by_importance(stores_df, bus_stops_df, stop_importance_levels):\n",
    "    \"\"\"\n",
    "    Distance for each store to the closest bus stop of each importance_level\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for importance_level in stop_importance_levels:\n",
    "        importance_level_cleaned = importance_level.lower().replace(\" \", \"_\")\n",
    "        df = bus_stops_closest(stores_df, bus_stops_df, importance_level=importance_level)\n",
    "        df.rename(columns={'distance': f'distance_to_{importance_level_cleaned}'}, inplace=True)\n",
    "        df_list.append(df[['store_id', f'distance_to_{importance_level_cleaned}']])\n",
    "\n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "def bus_stops_in_radius_by_importance(stores_df, bus_stops_df, stop_importance_levels, radius=0.01):\n",
    "    \"\"\"\n",
    "    Number of bus stops in radius of store for each importance level.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    df_list.append(bus_stops_in_radius(stores_df, bus_stops_df, radius=radius).rename(columns={'count':'number_of_all_stop_types'})) # All bus stops in radius\n",
    "    \n",
    "    for importance_level in stop_importance_levels:\n",
    "        importance_level_cleaned = importance_level.lower().replace(\" \", \"_\")\n",
    "        df = bus_stops_in_radius(stores_df, bus_stops_df, importance_level=importance_level, radius=radius)\n",
    "        df.rename(columns={'count': f'number_of_{importance_level_cleaned}'}, inplace=True)\n",
    "        df_list.append(df[['store_id', f'number_of_{importance_level_cleaned}']])\n",
    "\n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "def population(dataset_age):\n",
    "    population = dataset_age.drop([\"grunnkrets_id\"], axis=1).sum(axis=1)\n",
    "    dataset_age[\"population_count\"] = population\n",
    "    return dataset_age[[\"grunnkrets_id\", \"population_count\"]]\n",
    "\n",
    "def population_grouped(data_age, data_geography, grouping_element):\n",
    "    age_df = population(data_age)\n",
    "    geography_df = data_geography\n",
    "    population_df = age_df.merge(geography_df, how=\"left\", on=\"grunnkrets_id\")\n",
    "    grouped_df = population_df.groupby([grouping_element], as_index=False)[\"population_count\"].sum()\n",
    "    return grouped_df\n",
    "\n",
    "def population_count_grouped_by_geo_group(stores_df, age_df, grunnkrets_df, geo_groups): \n",
    "    combined_df = stores_df.merge(grunnkrets_df, how = \"left\", on = \"grunnkrets_id\")\n",
    "\n",
    "    population_columns = [\"population_count\"]\n",
    "    df_list = []\n",
    "\n",
    "    for geo_group in geo_groups: \n",
    "        pop_df = population_grouped(age_df, grunnkrets_df, geo_group)\n",
    "        merged_df = combined_df.merge(pop_df, how = \"left\", on = geo_group)[[\"store_id\"] + population_columns]\n",
    "        merged_df.set_index(\"store_id\", inplace = True)\n",
    "        merged_df2 = merged_df.add_prefix(f'{geo_group}_')\n",
    "        df_list.append(merged_df2)\n",
    "\n",
    "    return pd.concat(df_list, axis = 1).reset_index()\n",
    "\n",
    "def population_density(age_df, geo_df, grouping_element):\n",
    "    age_data = population(age_df)\n",
    "    geo_df = geo_df\n",
    "    combined_df = age_data.merge(geo_df, how=\"left\", on=\"grunnkrets_id\")\n",
    "    density_df = combined_df.groupby([grouping_element], as_index=False)[\n",
    "        [\"population_count\", \"area_km2\"]].sum()\n",
    "    density_df[\"density\"] = density_df[\"population_count\"] / \\\n",
    "        density_df[\"area_km2\"]\n",
    "    return density_df\n",
    "\n",
    "def population_density_grouped_by_geo_group(stores_df, age_df, grunnkrets_df, geo_groups):\n",
    "    grunnkrets_df_2016 = grunnkrets_df\n",
    "    combined_df = stores_df.merge(grunnkrets_df_2016, how = \"left\", on = \"grunnkrets_id\")\n",
    "\n",
    "    pop_density_columns = [\"density\"]\n",
    "    df_list = []\n",
    "\n",
    "    for geo_group in geo_groups: \n",
    "        pop_df = population_density(age_df, grunnkrets_df, geo_group)\n",
    "        merged_df = combined_df.merge(pop_df, how = \"left\", on = geo_group)[[\"store_id\"] + pop_density_columns]\n",
    "        merged_df.set_index(\"store_id\", inplace = True)\n",
    "        merged_df2 = merged_df.add_prefix(f'{geo_group}_')\n",
    "        df_list.append(merged_df2)\n",
    "\n",
    "    return pd.concat(df_list, axis = 1).reset_index()\n",
    "\n",
    "def age_distrubution(grunnkrets_age_df, geographic_df, grouping_element):\n",
    "    age_df1 = grunnkrets_age_df\n",
    "    age_df1[\"num_kids\"] = age_df1.iloc[:, 1:8].sum(axis=1)\n",
    "    age_df1[\"num_kids+\"] = age_df1.iloc[:, 8:14].sum(axis=1)\n",
    "    age_df1[\"num_youths\"] = age_df1.iloc[:, 14: 19].sum(axis=1)\n",
    "    age_df1[\"num_youthAdult\"] = age_df1.iloc[:, 19:27].sum(axis=1)\n",
    "    age_df1[\"num_adult\"] = age_df1.iloc[:, 27:37].sum(axis=1)\n",
    "    age_df1[\"num_adults+\"] = age_df1.iloc[:, 37:62].sum(axis=1)\n",
    "    age_df1[\"num_pensinors\"] = age_df1.iloc[:, 62:92].sum(axis=1)\n",
    "\n",
    "    age_df2 = age_df1[[\"grunnkrets_id\", \"num_kids\", \"num_kids+\", \"num_youths\",\n",
    "                       \"num_youthAdult\", \"num_adult\", \"num_adults+\", \"num_pensinors\"]]\n",
    "\n",
    "    pop_df = population(grunnkrets_age_df)\n",
    "    new_geo_df = geographic_df.drop([\"geometry\", \"area_km2\"], axis=1)\n",
    "    combined_df = age_df2.merge(pop_df, how=\"inner\", on=\"grunnkrets_id\").merge(\n",
    "        new_geo_df, how=\"inner\", on=\"grunnkrets_id\")\n",
    "    list_columns = [\"num_kids\", \"num_kids+\", \"num_youths\",\n",
    "                    \"num_youthAdult\", \"num_adult\", \"num_adults+\", \"num_pensinors\"]\n",
    "    combined_df2 = combined_df.groupby([grouping_element], as_index=False)[\n",
    "        list_columns].sum()\n",
    "\n",
    "    pop_gk = population_grouped(\n",
    "        grunnkrets_age_df, geographic_df, grouping_element)\n",
    "    new_df = combined_df2.merge(pop_gk, how=\"inner\", on=grouping_element)\n",
    "\n",
    "    new_df[\"kids_%\"] = new_df[\"num_kids\"] / new_df[\"population_count\"]\n",
    "    new_df[\"kids+_%\"] = new_df[\"num_kids+\"] / new_df[\"population_count\"]\n",
    "    new_df[\"youths_%\"] = new_df[\"num_youths\"] / new_df[\"population_count\"]\n",
    "    new_df[\"youthAdult_%\"] = new_df[\"num_youthAdult\"] / \\\n",
    "        new_df[\"population_count\"]\n",
    "    new_df[\"adult_%\"] = new_df[\"num_adult\"] / new_df[\"population_count\"]\n",
    "    new_df[\"adults+_%\"] = new_df[\"num_adults+\"] / new_df[\"population_count\"]\n",
    "    new_df[\"pensinors_%\"] = new_df[\"num_pensinors\"] / \\\n",
    "        new_df[\"population_count\"]\n",
    "\n",
    "    age_dist_df = new_df.drop([\"population_count\"]+[\"num_kids\", \"num_kids+\", \"num_youths\",\n",
    "                       \"num_youthAdult\", \"num_adult\", \"num_adults+\", \"num_pensinors\"], axis=1)\n",
    "    return age_dist_df\n",
    "\n",
    "def age_dist_by_geo_group(stores_df, age_df, grunnkrets_norway_df, geo_groups): \n",
    "    combined_df = stores_df.merge(grunnkrets_norway_df, how = \"left\", on = \"grunnkrets_id\")\n",
    "\n",
    "    age_columns = ['kids_%', 'kids+_%', 'youths_%',\n",
    "       'youthAdult_%', 'adult_%', 'adults+_%', 'pensinors_%']\n",
    "\n",
    "    df_list = []\n",
    "    for geo_group in geo_groups: \n",
    "      age_dist_df = age_distrubution(age_df, grunnkrets_norway_df, geo_group)\n",
    "      merged_df = combined_df.merge(age_dist_df, how = \"left\", on = geo_group)[[\"store_id\"] + age_columns]\n",
    "      merged_df.set_index(\"store_id\", inplace = True)\n",
    "      merged_df2 = merged_df.add_prefix(f'{geo_group}_')\n",
    "      df_list.append(merged_df2)\n",
    "    \n",
    "    return pd.concat(df_list, axis = 1).reset_index()\n",
    "\n",
    "def household_type_distrubution(grunnkrets_norway_df, grunnkrets_household_pop_df, grouping_element):\n",
    "    combined_df = grunnkrets_norway_df.merge(grunnkrets_household_pop_df, how=\"inner\", on=\"grunnkrets_id\")\n",
    "\n",
    "    list_columns = [\"couple_children_0_to_5_years\", \"couple_children_18_or_above\", \"couple_children_6_to_17_years\",\n",
    "                    \"couple_without_children\", \"single_parent_children_0_to_5_years\", \"single_parent_children_18_or_above\",\n",
    "                    \"single_parent_children_6_to_17_years\", \"singles\"]\n",
    "\n",
    "    grouped_df = combined_df.groupby([grouping_element], as_index=False)[\n",
    "        list_columns].sum()\n",
    "    grouped_df[\"tot_pop_count\"] = grouped_df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "    grouped_df[\"%_dist_of_couple_children_0_to_5_years\"] = grouped_df[\"couple_children_0_to_5_years\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_couple_children_18_or_above\"] = grouped_df[\"couple_children_18_or_above\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_couple_children_6_to_17_years\"] = grouped_df[\"couple_children_6_to_17_years\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_couple_without_children\"] = grouped_df[\"couple_without_children\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_single_parent_children_0_to_5_years\"] = grouped_df[\"single_parent_children_0_to_5_years\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_single_parent_children_18_or_above\"] = grouped_df[\"single_parent_children_18_or_above\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_single_parent_children_6_to_17_years\"] = grouped_df[\"single_parent_children_6_to_17_years\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "    grouped_df[\"%_dist_of_singles\"] = grouped_df[\"singles\"] / \\\n",
    "        grouped_df[\"tot_pop_count\"]\n",
    "\n",
    "    returned_df = grouped_df.drop([\"tot_pop_count\"], axis=1)\n",
    "    return returned_df\n",
    "\n",
    "def household_dist_by_geo_group(stores_df, grunnkrets_household_pop_df, grunnkrets_norway_df, geo_groups):\n",
    "    combined_df = stores_df.merge(grunnkrets_norway_df, how = \"left\", on = \"grunnkrets_id\")\n",
    "    \n",
    "    household_columns = ['couple_children_0_to_5_years', 'couple_children_18_or_above', 'couple_children_6_to_17_years', 'couple_without_children',\n",
    "       'single_parent_children_0_to_5_years','single_parent_children_18_or_above','single_parent_children_6_to_17_years', 'singles',\n",
    "       '%_dist_of_couple_children_0_to_5_years','%_dist_of_couple_children_18_or_above','%_dist_of_couple_children_6_to_17_years',\n",
    "       '%_dist_of_couple_without_children','%_dist_of_single_parent_children_0_to_5_years','%_dist_of_single_parent_children_18_or_above',\n",
    "       '%_dist_of_single_parent_children_6_to_17_years', '%_dist_of_singles']\n",
    "       \n",
    "    df_list = []\n",
    "\n",
    "    for geo_group in geo_groups: \n",
    "        household_type_df = household_type_distrubution(grunnkrets_norway_df, grunnkrets_household_pop_df, geo_group)\n",
    "        merged_df = combined_df.merge(household_type_df, how = \"left\", on = geo_group)[[\"store_id\"] + household_columns]\n",
    "        merged_df.set_index(\"store_id\", inplace = True)\n",
    "        merged_df2 = merged_df.add_prefix(f'{geo_group}_')\n",
    "        df_list.append(merged_df2)\n",
    "    return pd.concat(df_list, axis = 1)\n",
    "\n",
    "def mean_income_per_capita(grunnkrets_age_df, grunnkrets_household_inc_df):\n",
    "    \"mean income per capita per grunnkrets\"\n",
    "    age_df = population(grunnkrets_age_df)\n",
    "    age_and_income_df = age_df.merge(grunnkrets_household_inc_df, how='left', on='grunnkrets_id')\n",
    "    mean_income = age_and_income_df.drop(['singles', 'couple_without_children',\n",
    "                                         'couple_with_children', 'other_households', 'single_parent_with_children'], axis=1)\n",
    "    mean_income['mean_income'] = mean_income['all_households'] / \\\n",
    "        mean_income['population_count']\n",
    "    mean_income = mean_income.drop(['all_households'], axis=1)\n",
    "\n",
    "    return mean_income\n",
    "\n",
    "def mean_income_per_capita_grouped(grunnkrets_age_df, grunnkrets_household_inc_df, grunnkrets_norway_df, geo_group, agg_name):\n",
    "    # gets data from mean_income_per_capita functino\n",
    "    data_mean_income = mean_income_per_capita(grunnkrets_age_df, grunnkrets_household_inc_df)\n",
    "    # gets data from geography set and makes sure we only use data for 2016\n",
    "    # gets the data of mean income with the geography data\n",
    "    mean_income_geo_df = data_mean_income.merge(\n",
    "        grunnkrets_norway_df, how='left', on='grunnkrets_id')\n",
    "    # sum the number of people based on grouping element\n",
    "    grouped_population_df = mean_income_geo_df.groupby(\n",
    "        [geo_group], as_index=False)[\"population_count\"].sum()\n",
    "    # merge this with the grunnkrets to see both total population per selected area and grunnkrets\n",
    "    total_grouped_df = mean_income_geo_df.merge(\n",
    "        grouped_population_df, how='left', on=geo_group)\n",
    "    portion_income_df = total_grouped_df\n",
    "    # find ration of grunnkrets to total population and multiply this with grunnkrets mean income\n",
    "    portion_income_df['mean_income'] = total_grouped_df['mean_income'] * \\\n",
    "        total_grouped_df['population_count_x'] / \\\n",
    "        total_grouped_df['population_count_y']\n",
    "    # add these incomes together, should add up to the total mean income for the selected area\n",
    "    grouped_income_df = portion_income_df.groupby(\n",
    "        [geo_group])[\"mean_income\"].sum().reset_index(name=agg_name)\n",
    "    return grouped_income_df\n",
    "\n",
    "def mean_income_per_capita_by_geo_group(stores_df, grunnkrets_age_df, grunnkrets_household_inc_df, grunnkrets_norway_df, geo_groups):\n",
    "    merged_df = stores_df.merge(grunnkrets_norway_df, how=\"left\", on=\"grunnkrets_id\")\n",
    "    \n",
    "    df_list = []\n",
    "    for geo_group in geo_groups:\n",
    "        df = mean_income_per_capita_grouped(grunnkrets_age_df, grunnkrets_household_inc_df, grunnkrets_norway_df, geo_group, agg_name=f'{geo_group}_mean_income')\n",
    "        df_list.append(merged_df.merge(df, how=\"left\", on=[geo_group])[['store_id', f'{geo_group}_mean_income']])\n",
    "    \n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "def num_households(household_dist):\n",
    "    household_dist = household_dist.copy()\n",
    "    population = household_dist.drop([\"grunnkrets_id\"], axis=1).sum(axis=1)\n",
    "    household_dist[\"household_count\"] = population\n",
    "    return household_dist[[\"grunnkrets_id\", \"household_count\"]]\n",
    "\n",
    "def num_households_geo(geo_group, household_dist, grunnkrets_df):\n",
    "    _num_households = num_households(household_dist)\n",
    "    merged_df = grunnkrets_df.merge(_num_households, on=\"grunnkrets_id\", how=\"inner\")\n",
    "    return merged_df.groupby([geo_group], as_index=False)['household_count'].sum()\n",
    "\n",
    "def total_grunnkrets_income(income_dist, household_dist):\n",
    "    _num_households = num_households(household_dist)\n",
    "    merged_df = income_dist.merge(_num_households, on=\"grunnkrets_id\", how=\"inner\")[['grunnkrets_id', 'household_count', 'all_households']]\n",
    "    merged_df['total_income'] = merged_df['household_count'] * merged_df['all_households']\n",
    "    return merged_df[['grunnkrets_id', 'total_income']]    \n",
    "\n",
    "def total_income_geo(geo_group, income_dist, household_dist, grunnkrets_df):\n",
    "    grunnkrets_income = total_grunnkrets_income(income_dist, household_dist)\n",
    "    merged_df = grunnkrets_df.merge(grunnkrets_income, on=\"grunnkrets_id\", how=\"inner\")\n",
    "    return merged_df.groupby([geo_group], as_index=False)['total_income'].sum()\n",
    "\n",
    "def average_household_income_geo(geo_group, income_dist, household_dist, grunnkrets_df):\n",
    "    income = total_income_geo(geo_group, income_dist, household_dist, grunnkrets_df)\n",
    "    households = num_households_geo(geo_group, household_dist, grunnkrets_df)\n",
    "    \n",
    "    merged_df = income.merge(households, on=geo_group, how=\"inner\")\n",
    "    merged_df[f'avg_household_income_{geo_group}'] = merged_df['total_income'] / merged_df['household_count']\n",
    "    return merged_df[[geo_group, f'avg_household_income_{geo_group}']]\n",
    "    \n",
    "def average_household_income_by_geo_groups(stores_df, geo_groups, income_dist, household_dist, grunnkrets_df):\n",
    "    merged_df = stores_df.merge(grunnkrets_df, how=\"left\", on=\"grunnkrets_id\")\n",
    "    \n",
    "    df_list = []\n",
    "    for geo_group in geo_groups:\n",
    "        df = average_household_income_geo(geo_group, income_dist, household_dist, grunnkrets_df)\n",
    "        df_list.append(merged_df.merge(df, how=\"left\", on=[geo_group])[['store_id', f'avg_household_income_{geo_group}']])\n",
    "    \n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1).reset_index()\n",
    "\n",
    "def stores_in_radius(stores_df, compare_df, radius=0.1, store_type_group=None):\n",
    "    mat = cdist(stores_df[['lat', 'lon']],\n",
    "                compare_df[['lat', 'lon']], metric=\"euclidean\")\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=compare_df['store_id']\n",
    "    )\n",
    "    \n",
    "    if store_type_group is None:\n",
    "        count = new_df[(new_df < radius) & (new_df > 0)].count(axis=1)\n",
    "        return count.to_frame(name=\"all_stores_in_radius\").reset_index()\n",
    "    \n",
    "    else:\n",
    "        test_df = new_df[(new_df < radius) & (new_df > 0)]\n",
    "        store_count = {}\n",
    "        \n",
    "        for index, row in test_df.iterrows():\n",
    "            nearby_stores = row.dropna().index.values\n",
    "            index_type = compare_df.loc[compare_df['store_id'] == index, store_type_group].iat[0]\n",
    "            \n",
    "            number_same = compare_df[(compare_df['store_id'].isin(nearby_stores)) & (\n",
    "                compare_df[store_type_group] == index_type)]['store_id'].count()\n",
    "            \n",
    "            store_count[index] = number_same\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(store_count, orient='index', columns=[f\"{store_type_group}_in_radius\"])\n",
    "        df.index.rename('store_id', inplace=True)\n",
    "        return df.reset_index()\n",
    "\n",
    "def store_closest(stores_df, compare_df, store_type_group=\"lv4_desc\"):\n",
    "    \"\"\"\n",
    "    Id and distance of the closest store of same type in the same group.\n",
    "    \"\"\"\n",
    "    \n",
    "    store_types_in_group = stores_df[store_type_group].unique()\n",
    "    df_list = []\n",
    "    for store_type in store_types_in_group:\n",
    "        stores_by_type = stores_df[stores_df[store_type_group] == store_type]\n",
    "        stores_comp_by_type = compare_df[compare_df[store_type_group] == store_type]\n",
    "        \n",
    "        mat = cdist(stores_by_type[['lat', 'lon']], stores_comp_by_type[['lat', 'lon']], metric='euclidean')\n",
    "        \n",
    "        df = pd.DataFrame(\n",
    "            mat, index=stores_by_type['store_id'], columns=stores_comp_by_type['store_id'])\n",
    "        \n",
    "        df = df[df > 0]\n",
    "        \n",
    "        stores = df.index\n",
    "        closest = df.idxmin(axis=1)\n",
    "        distance = df.min(axis=1)\n",
    "        \n",
    "        new_df = pd.DataFrame({'store_id': stores.values, 'closest_store': closest.values, 'distance': distance.values})\n",
    "        df_list.append(new_df)\n",
    "        \n",
    "    \n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def store_closest_by_store_groups(stores_df, compare_df, store_type_groups):\n",
    "    df_list = []\n",
    "    \n",
    "    for store_type_group in store_type_groups:\n",
    "        df = store_closest(stores_df, compare_df, store_type_group=store_type_group)\n",
    "        df.rename(columns={'distance': f'distance_to_{store_type_group}'}, inplace=True)\n",
    "        df_list.append(df[['store_id', f'distance_to_{store_type_group}']])\n",
    "\n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1).reset_index()\n",
    "\n",
    "def encode_levels(stores_df):\n",
    "    stores_df['level2'] = stores_df['lv1'].astype(str) + \",\" + stores_df['lv2'].astype(str)\n",
    "    stores_df['level3'] = stores_df['lv1'].astype(str) + \",\" + stores_df['lv2'].astype(str) + \",\" +stores_df['lv3'].astype(str)\n",
    "    \n",
    "    return stores_df\n",
    "\n",
    "\n",
    "def new_age_dist(stores_df, age_df, grunnkrets_df, geo_groups):\n",
    "    _age_dist = age_dist_by_geo_group(stores_df, age_df, grunnkrets_df, geo_groups)\n",
    "    return _age_dist.fillna(_age_dist.mean())\n",
    "\n",
    "def new_pop_density(stores_df, age_dist, grunnkrets_df, geo_groups):\n",
    "    population_density = population_density_grouped_by_geo_group(stores_df, age_dist, grunnkrets_df, geo_groups)\n",
    "    return population_density.fillna(population_density.mean())\n",
    "\n",
    "def stores_in_radius_new(stores_merged, compare_df, radius=0.05):\n",
    "    lv_1 = stores_in_radius(stores_merged, compare_df, radius=radius, store_type_group='lv1_desc')\n",
    "    lv_2 = stores_in_radius(stores_merged, compare_df, radius=radius, store_type_group='lv2_desc')\n",
    "    lv_3 = stores_in_radius(stores_merged, compare_df, radius=radius, store_type_group='lv3_desc')\n",
    "    lv_4 = stores_in_radius(stores_merged, compare_df, radius=radius, store_type_group='lv4_desc')\n",
    "    all_count = stores_in_radius(stores_merged, compare_df, radius=radius, store_type_group=None)\n",
    "    \n",
    "    return lv_1.merge(lv_2, on=\"store_id\", how=\"inner\").merge(lv_3, on=\"store_id\", how=\"inner\").merge(lv_4, on=\"store_id\", how=\"inner\").merge(all_count, on=\"store_id\", how=\"inner\")\n",
    "\n",
    "def distance_to_closest_group(stores_df, compare_df, group):\n",
    "    \"\"\"\n",
    "        Mall or chain\n",
    "    \"\"\"\n",
    "    mat = cdist(\n",
    "        stores_df[['lat', 'lon']],\n",
    "        compare_df[compare_df[group].notna()][['lat', 'lon']], metric=\"euclidean\"\n",
    "    )\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=compare_df[compare_df[group].notna()]['store_id']\n",
    "    )\n",
    "    \n",
    "    new_df = new_df[new_df > 0]\n",
    "    \n",
    "    stores = new_df.index\n",
    "    # closest = new_df.idxmin(axis=1)\n",
    "    distance = new_df.min(axis=1)\n",
    "    \n",
    "    return pd.DataFrame({'store_id': stores.values, f'distance_closest_{group}': distance.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/318296919.py:25: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  bus_stops_df[['lon', 'lat']] = bus_stops_df['lng_lat'].str.split(\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/\"\n",
    "\n",
    "raw_stores_train = raw_read(raw_path, \"stores_train\", {'grunnkrets_id':str})\n",
    "raw_stores_test = raw_read(raw_path, \"stores_test\", {'grunnkrets_id':str})\n",
    "raw_stores_extra = raw_read(raw_path,\"stores_extra\", {'grunnkrets_id':str})\n",
    "\n",
    "raw_income_dist = raw_read(raw_path, \"grunnkrets_income_households\", {'grunnkrets_id':str})\n",
    "raw_age_dist = raw_read(raw_path, \"grunnkrets_age_distribution\", {'grunnkrets_id':str})\n",
    "raw_households_dist = raw_read(raw_path, \"grunnkrets_households_num_persons\", {'grunnkrets_id':str})\n",
    "raw_grunnkrets = raw_read(raw_path, \"grunnkrets_norway_stripped\", {'grunnkrets_id':str})\n",
    "\n",
    "raw_plaace = raw_read(raw_path, \"plaace_hierarchy\", {'lv1':str, 'lv2':str})\n",
    "raw_bus_stops = raw_read(raw_path, \"busstops_norway\")\n",
    "\n",
    "dedup_income_dist = deduplicate_year(raw_income_dist)\n",
    "dedup_age_dist = deduplicate_year(raw_age_dist)\n",
    "dedup_households_dist = deduplicate_year(raw_households_dist)\n",
    "dedup_grunnkrets = combine_keys(deduplicate_year(raw_grunnkrets))\n",
    "\n",
    "enriched_bus_stops = bus_stops_lat_lon(raw_bus_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train_merged = raw_stores_train.merge(raw_plaace, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "stores_test_merged = raw_stores_test.merge(raw_plaace, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "stores_extra_merged = raw_stores_extra.merge(raw_plaace, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "\n",
    "compare_train_df = pd.concat([stores_train_merged, stores_extra_merged], ignore_index=True)\n",
    "compare_test_df = pd.concat([stores_test_merged, stores_extra_merged], ignore_index=True)\n",
    "\n",
    "stores_types = ['lv1_desc', 'lv2_desc', 'lv3_desc', 'lv4_desc']\n",
    "geo_groups = ['grunnkrets_id', 't_district', 'municipality_name']\n",
    "stop_importance_levels = ['Mangler viktighetsnivå',\n",
    "                          'Standard holdeplass',\n",
    "                          'Lokalt knutepunkt',\n",
    "                          'Nasjonalt knutepunkt',\n",
    "                          'Regionalt knutepunkt',\n",
    "                          'Annen viktig holdeplass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = stores_train_merged\n",
    "test_df = stores_test_merged \n",
    "\n",
    "def firstname(string): \n",
    "    if \" \" in string: \n",
    "        newstring = string.split(\" \")[:-1]\n",
    "        return \" \".join(newstring)\n",
    "    else: \n",
    "        return string\n",
    "\n",
    "def lastname(string):\n",
    "    if \" \" in string: \n",
    "        if string.split(\" \")[-1] == 'AS': \n",
    "            return string.split(\" \")[-2]\n",
    "        else: \n",
    "            return string.split(\" \")[-1]  \n",
    "    else: \n",
    "        return string\n",
    "\n",
    "\n",
    "def address(string): \n",
    "    newstring = \"\"\n",
    "    for char in string: \n",
    "        if not char.isdigit(): \n",
    "            newstring += char\n",
    "        else: \n",
    "            return newstring\n",
    "\n",
    "def district(string): \n",
    "    string = string.lower()\n",
    "    if 'sentrum' in string:\n",
    "        return 'sentrum'\n",
    "    else: \n",
    "        return string\n",
    "\n",
    "train_df['lastname'] = train_df[\"store_name\"].apply(lastname)\n",
    "test_df['lastname'] = test_df[\"store_name\"].apply(lastname)\n",
    "\n",
    "train_df['firstname'] = train_df[\"store_name\"].apply(firstname)\n",
    "test_df['firstname'] = test_df[\"store_name\"].apply(firstname)\n",
    "\n",
    "train_df['AS'] = (train_df['lastname'] == 'AS').astype(str)\n",
    "\n",
    "train_df['lastname'] = train_df[\"store_name\"].apply(lastname)\n",
    "\n",
    "test_df['AS'] = (test_df['lastname'] == 'AS').astype(str)\n",
    "\n",
    "test_df['lastname'] = test_df[\"store_name\"].apply(lastname)\n",
    "\n",
    "\n",
    "train_df['last_count'] = train_df.groupby('lastname')['lastname'].transform('count')\n",
    "train_df['first_count'] = train_df.groupby('firstname')['firstname'].transform('count')\n",
    "\n",
    "test_df['last_count'] = test_df.groupby('lastname')['lastname'].transform('count')\n",
    "test_df['first_count'] = test_df.groupby('firstname')['firstname'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/4240257991.py:392: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return population_density.fillna(population_density.mean())\n",
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/4240257991.py:388: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return _age_dist.fillna(_age_dist.mean())\n",
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/4240257991.py:392: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return population_density.fillna(population_density.mean())\n",
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/4240257991.py:388: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return _age_dist.fillna(_age_dist.mean())\n"
     ]
    }
   ],
   "source": [
    "merged_stores_train = train_df \\\n",
    "    .merge(dedup_grunnkrets, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(dedup_income_dist, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(store_closest_by_store_groups(stores_train_merged, compare_train_df, stores_types), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(stores_in_radius_new(stores_train_merged, compare_train_df), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_pop_density(raw_stores_train, dedup_age_dist, dedup_grunnkrets, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(average_household_income_by_geo_groups(raw_stores_train, geo_groups, raw_income_dist, dedup_households_dist, dedup_grunnkrets)) \\\n",
    "    .merge(bus_stops_distance_by_importance(raw_stores_train, enriched_bus_stops, stop_importance_levels).reset_index(level=0), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_age_dist(raw_stores_train, dedup_age_dist, dedup_grunnkrets, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(distance_to_closest_group(stores_train_merged, compare_train_df, \"mall_name\"), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(distance_to_closest_group(stores_train_merged, compare_train_df, \"chain_name\"), on=\"store_id\", how=\"left\") \n",
    "    \n",
    "merged_stores_test = test_df \\\n",
    "    .merge(dedup_grunnkrets, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(dedup_income_dist, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(store_closest_by_store_groups(stores_test_merged, compare_test_df, stores_types), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(stores_in_radius_new(stores_test_merged, compare_test_df), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_pop_density(raw_stores_test, dedup_age_dist, dedup_grunnkrets, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(average_household_income_by_geo_groups(raw_stores_test, geo_groups, raw_income_dist, dedup_households_dist, dedup_grunnkrets)) \\\n",
    "    .merge(bus_stops_distance_by_importance(raw_stores_test, enriched_bus_stops, stop_importance_levels).reset_index(level=0), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_age_dist(raw_stores_test, dedup_age_dist, dedup_grunnkrets, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(distance_to_closest_group(stores_test_merged, compare_test_df, \"mall_name\"), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(distance_to_closest_group(stores_test_merged, compare_test_df, \"chain_name\"), on=\"store_id\", how=\"left\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = merged_stores_train[['store_id', 'revenue']].copy()\n",
    "merged_stores_train = merged_stores_train.copy().drop('revenue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stores_train[\"lv2\"] = merged_stores_train[\"lv2\"].apply(lambda x: x.replace(\".\",\"_\"))\n",
    "merged_stores_train[\"lv3\"] = merged_stores_train[\"lv3\"].apply(lambda x: x.replace(\".\",\"_\"))\n",
    "merged_stores_train[\"lv4\"] = merged_stores_train[\"lv4\"].apply(lambda x: x.replace(\".\",\"_\"))\n",
    "\n",
    "merged_stores_test[\"lv2\"] = merged_stores_test[\"lv2\"].apply(lambda x: x.replace(\".\",\"_\"))\n",
    "merged_stores_test[\"lv3\"] = merged_stores_test[\"lv3\"].apply(lambda x: x.replace(\".\",\"_\"))\n",
    "merged_stores_test[\"lv4\"] = merged_stores_test[\"lv4\"].apply(lambda x: x.replace(\".\",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_cols = [\n",
    "    'store_id',\n",
    "    \"plaace_hierarchy_id\",\n",
    "    'chain_name',\n",
    "    'mall_name',\n",
    "    'sales_channel_name_x',\n",
    "    'municipality_name',\n",
    "    \"grunnkrets_id\",\n",
    "    \"t_district\",\n",
    "    \"grunnkrets_name\",\n",
    "    \"address\",\n",
    "    'lv1',\n",
    "    'lv2',\n",
    "    'lv3',\n",
    "    'lv4',  \n",
    "    \n",
    "    'firstname',\n",
    "    'lastname',\n",
    "    'AS',\n",
    "]\n",
    "\n",
    "yeo_cols = [\n",
    "    \"lv1_desc_in_radius\",\n",
    "    \"lv2_desc_in_radius\",\n",
    "    \"lv3_desc_in_radius\",\n",
    "    \"lv4_desc_in_radius\",\n",
    "    \"all_stores_in_radius\", \n",
    "    \"all_households\",\n",
    "    \"singles\",\n",
    "    \"couple_without_children\",\n",
    "    \"couple_with_children\",\n",
    "    \"other_households\",\n",
    "    \"single_parent_with_children\",\n",
    "    'last_count',\n",
    "    'first_count',  \n",
    "]\n",
    "\n",
    "box_cols = [\n",
    "    \"distance_closest_mall_name\",\n",
    "    \"distance_closest_chain_name\",\n",
    "    \"distance_to_lv1_desc\",\n",
    "    \"distance_to_lv2_desc\",\n",
    "    \"distance_to_lv3_desc\",\n",
    "    \"distance_to_lv4_desc\",\n",
    "    'distance_to_mangler_viktighetsnivå',\n",
    "    'distance_to_standard_holdeplass',\n",
    "    'distance_to_lokalt_knutepunkt',\n",
    "    'distance_to_nasjonalt_knutepunkt',\n",
    "    'distance_to_regionalt_knutepunkt',\n",
    "    'distance_to_annen_viktig_holdeplass',\n",
    "    \n",
    "    \"grunnkrets_id_density\",\n",
    "    \"t_district_density\",\n",
    "    \"municipality_name_density\", \n",
    "]\n",
    "\n",
    "_merged_stores_train = merged_stores_train.filter(inc_cols+yeo_cols+box_cols)\n",
    "_merged_stores_test = merged_stores_test.filter(inc_cols+yeo_cols+box_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PowerTransformer()\n",
    ")\n",
    "box_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PowerTransformer(method=\"box-cox\")\n",
    ")\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "        (yeo_pipeline, yeo_cols),\n",
    "        (box_pipeline, box_cols),\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "def new_transformer(merged_stores_df, preprocessing):\n",
    "    return pd.DataFrame(preprocessing.fit_transform(merged_stores_df), columns=preprocessing.get_feature_names_out(), index=merged_stores_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_stores_train = new_transformer(_merged_stores_train, preprocessing)\n",
    "_merged_stores_test = new_transformer(_merged_stores_test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer()\n",
    "rev_transformed = pt.fit_transform(target_labels[[\"revenue\"]])\n",
    "_merged_stores_train[\"revenue\"] = rev_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_stores_train = _merged_stores_train[(_merged_stores_train.revenue > -1.888)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>1 hour 32 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Oslo</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.38.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>17 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_nwong_ekvwsk</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>2.736 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.4 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         1 hour 32 mins\n",
       "H2O_cluster_timezone:       Europe/Oslo\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.38.0.2\n",
       "H2O_cluster_version_age:    17 days\n",
       "H2O_cluster_name:           H2O_from_python_nwong_ekvwsk\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    2.736 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.4 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "train = h2o.H2OFrame(_merged_stores_train)\n",
    "test = h2o.H2OFrame(_merged_stores_test)\n",
    "\n",
    "cat_vars = inc_cols\n",
    "\n",
    "cat_vars = [f'remainder__{i}' for i in cat_vars if i != 'store_id']\n",
    "\n",
    "for cat in cat_vars:\n",
    "    train[cat] = train[cat].asfactor()\n",
    "    test[cat] = test[cat].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |\n",
      "22:59:32.233: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "███\n",
      "22:59:49.40: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "███\n",
      "23:00:10.910: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "\n",
      "23:00:15.693: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "██\n",
      "23:00:28.487: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "█████████\n",
      "23:03:17.757: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "23:03:21.872: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "█\n",
      "23:03:27.683: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "██\n",
      "23:03:38.932: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "█\n",
      "23:03:48.398: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "██\n",
      "23:05:35.183: _train param, Dropping bad and constant columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "█████████████████████████\n",
      "23:07:13.487: _train param, Dropping unused columns: [remainder__AS, remainder__store_id]\n",
      "23:07:14.124: _train param, Dropping unused columns: [remainder__AS, remainder__store_id]\n",
      "\n",
      "███████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_1_AutoML_2_20221113_225932\n",
       "\n",
       "No summary for this model</pre>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.27149958662737406\n",
       "RMSE: 0.5210562221367039\n",
       "MAE: 0.3991086017053688\n",
       "RMSLE: NaN\n",
       "Mean Residual Deviance: 0.27149958662737406\n",
       "R^2: 0.7141599655142957\n",
       "Null degrees of freedom: 10052\n",
       "Residual degrees of freedom: 10035\n",
       "Null deviance: 9548.648214687171\n",
       "Residual deviance: 2729.3853443649914\n",
       "AIC: 15460.130409914504</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 0.49147230419739174\n",
       "RMSE: 0.7010508570691514\n",
       "MAE: 0.540140764833516\n",
       "RMSLE: NaN\n",
       "Mean Residual Deviance: 0.49147230419739174\n",
       "R^2: 0.48446045327118237\n",
       "Null degrees of freedom: 12635\n",
       "Residual degrees of freedom: 12621\n",
       "Null deviance: 12049.767925719316\n",
       "Residual deviance: 6210.244035838242\n",
       "AIC: 26915.435921609693</pre></div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-6.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-6 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-6 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-6 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-6 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-6 .h2o-table th,\n",
       "#h2o-table-6 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Cross-Validation Metrics Summary: </caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>mean</th>\n",
       "<th>sd</th>\n",
       "<th>cv_1_valid</th>\n",
       "<th>cv_2_valid</th>\n",
       "<th>cv_3_valid</th>\n",
       "<th>cv_4_valid</th>\n",
       "<th>cv_5_valid</th></tr></thead>\n",
       "    <tbody><tr><td>mae</td>\n",
       "<td>0.5403110</td>\n",
       "<td>0.0049839</td>\n",
       "<td>0.536622</td>\n",
       "<td>0.5448842</td>\n",
       "<td>0.5464153</td>\n",
       "<td>0.5380719</td>\n",
       "<td>0.5355617</td></tr>\n",
       "<tr><td>mean_residual_deviance</td>\n",
       "<td>0.4914147</td>\n",
       "<td>0.0056905</td>\n",
       "<td>0.4853008</td>\n",
       "<td>0.4972618</td>\n",
       "<td>0.4965665</td>\n",
       "<td>0.4920937</td>\n",
       "<td>0.4858508</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>0.4914147</td>\n",
       "<td>0.0056905</td>\n",
       "<td>0.4853008</td>\n",
       "<td>0.4972618</td>\n",
       "<td>0.4965665</td>\n",
       "<td>0.4920937</td>\n",
       "<td>0.4858508</td></tr>\n",
       "<tr><td>null_deviance</td>\n",
       "<td>2409.9536</td>\n",
       "<td>78.479866</td>\n",
       "<td>2393.7407</td>\n",
       "<td>2432.2131</td>\n",
       "<td>2502.3323</td>\n",
       "<td>2433.1992</td>\n",
       "<td>2288.2827</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.4842008</td>\n",
       "<td>0.0057150</td>\n",
       "<td>0.4771154</td>\n",
       "<td>0.4801295</td>\n",
       "<td>0.4897929</td>\n",
       "<td>0.4840578</td>\n",
       "<td>0.4899082</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>1242.0419</td>\n",
       "<td>43.10652</td>\n",
       "<td>1251.5908</td>\n",
       "<td>1264.0394</td>\n",
       "<td>1275.6793</td>\n",
       "<td>1251.8864</td>\n",
       "<td>1167.0137</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.7010004</td>\n",
       "<td>0.0040599</td>\n",
       "<td>0.6966354</td>\n",
       "<td>0.7051679</td>\n",
       "<td>0.7046747</td>\n",
       "<td>0.7014939</td>\n",
       "<td>0.69703</td></tr>\n",
       "<tr><td>rmsle</td>\n",
       "<td>nan</td>\n",
       "<td>0.0</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td>\n",
       "<td>nan</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
      ],
      "text/plain": [
       "Model Details\n",
       "=============\n",
       "H2OStackedEnsembleEstimator : Stacked Ensemble\n",
       "Model Key: StackedEnsemble_AllModels_1_AutoML_2_20221113_225932\n",
       "\n",
       "No summary for this model\n",
       "\n",
       "ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.27149958662737406\n",
       "RMSE: 0.5210562221367039\n",
       "MAE: 0.3991086017053688\n",
       "RMSLE: NaN\n",
       "Mean Residual Deviance: 0.27149958662737406\n",
       "R^2: 0.7141599655142957\n",
       "Null degrees of freedom: 10052\n",
       "Residual degrees of freedom: 10035\n",
       "Null deviance: 9548.648214687171\n",
       "Residual deviance: 2729.3853443649914\n",
       "AIC: 15460.130409914504\n",
       "\n",
       "ModelMetricsRegressionGLM: stackedensemble\n",
       "** Reported on cross-validation data. **\n",
       "\n",
       "MSE: 0.49147230419739174\n",
       "RMSE: 0.7010508570691514\n",
       "MAE: 0.540140764833516\n",
       "RMSLE: NaN\n",
       "Mean Residual Deviance: 0.49147230419739174\n",
       "R^2: 0.48446045327118237\n",
       "Null degrees of freedom: 12635\n",
       "Residual degrees of freedom: 12621\n",
       "Null deviance: 12049.767925719316\n",
       "Residual deviance: 6210.244035838242\n",
       "AIC: 26915.435921609693\n",
       "\n",
       "Cross-Validation Metrics Summary: \n",
       "                        mean      sd          cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "----------------------  --------  ----------  ------------  ------------  ------------  ------------  ------------\n",
       "mae                     0.540311  0.00498387  0.536622      0.544884      0.546415      0.538072      0.535562\n",
       "mean_residual_deviance  0.491415  0.00569048  0.485301      0.497262      0.496566      0.492094      0.485851\n",
       "mse                     0.491415  0.00569048  0.485301      0.497262      0.496566      0.492094      0.485851\n",
       "null_deviance           2409.95   78.4799     2393.74       2432.21       2502.33       2433.2        2288.28\n",
       "r2                      0.484201  0.005715    0.477115      0.480129      0.489793      0.484058      0.489908\n",
       "residual_deviance       1242.04   43.1065     1251.59       1264.04       1275.68       1251.89       1167.01\n",
       "rmse                    0.701     0.00405987  0.696635      0.705168      0.704675      0.701494      0.69703\n",
       "rmsle                   nan       0           nan           nan           nan           nan           nan\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train.columns\n",
    "y = \"revenue\"\n",
    "x.remove(y)\n",
    "\n",
    "# Run AutoML for 20 base models\n",
    "aml = H2OAutoML(max_models=20, seed=1, exclude_algos=['deeplearning'])\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |███████████████████████████████████████████| (done) 100%\n",
      "stackedensemble prediction progress: |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__plaace_hierarchy_id' has levels not trained on: [\"1.5.1.0\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__chain_name' has levels not trained on: [\"24 7 TRENINGSSENTER\", \"ACE SHOPS\", \"AVANCIA SPORT\", \"BADEMILJØ\", \"BADERINGEN\", \"BAROKK FRISØR\", \"BERTONI NORWAY\", \"BIRK SPORT\", \"BLUE ENERGY\", \"DESINESS\", ...25 not listed..., \"SALT OG PEPPER HØNEFOSS\", \"SKOGSTAD DETALJ\", \"SKY FITNESS\", \"TIGER OF SWEDEN\", \"TIPPY\", \"VARMEFAG INVEST\", \"VITA EXCLUSIVE\", \"YOGIS\", \"ZAVANNA\", \"ZAXIZ FRISØR\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__mall_name' has levels not trained on: [\"Aamodt Senter\", \"Bardufoss Torgsenter\", \"Bellevuesenteret\", \"Borgeåsen Senter\", \"CC Mart'n\", \"Combisenteret Fiskå\", \"Coop Mega Hønefoss\", \"Coop Mega Øvre Årdal\", \"Cuben Kjøpesenter\", \"De Syv Søstre kjøpesenter\", ...39 not listed..., \"Strøget\", \"Sundt Motehus\", \"Svelvik Nærsenter\", \"Tebo Senter\", \"Trekanten Senter Kristiansand\", \"Tårnhuset\", \"Varehuset Rosendal\", \"Vik Torg\", \"Vossevangen Senter Mega\", \"Vågsenteret\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__sales_channel_name_x' has levels not trained on: [\"Bowling alleys\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__municipality_name' has levels not trained on: [\"Beiarn\", \"Bokn\", \"Gjemnes\", \"Høylandet\", \"Modalen\", \"Snillfjord\", \"Tranøy\", \"Unjárga - Nesseby\", \"Utsira\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__grunnkrets_id' has levels not trained on: [\"1010103\", \"1010110\", \"1010201\", \"1010302\", \"1010401\", \"1010404\", \"1040103\", \"1040206\", \"1040216\", \"1040411\", ...1132 not listed..., \"20230105\", \"20230107\", \"20240106\", \"20270107\", \"20280101\", \"20300101\", \"20300202\", \"20300205\", \"20300306\", \"20300311\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__t_district' has levels not trained on: [\"AlgarheimUllensaker\", \"BeiarnBeiarn\", \"Bersagel/HøleSandnes\", \"Bjoa/VikebygdVindafjord\", \"BlystadliaRælingen\", \"BoknBokn\", \"BordalenVoss\", \"BraskereidfossVåler\", \"Buksnes nordreVestvågøy\", \"BøverbruVestre Toten\", ...88 not listed..., \"Vest/nordSortland\", \"Vestre Gausdal nordGausdal\", \"Vestre PorsangerPorsanger - Porsángu - Porsanki\", \"Ytre AverøyAverøy\", \"Ytre GjemnesGjemnes\", \"Ytre Herøy 2Herøy\", \"Ytre VadsøVadsø\", \"Ålvundeid/ÅlvundfjordSunndal\", \"Årvik-Mauranger-ÆnesKvinnherad\", \"ÅsEidsvoll\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__grunnkrets_name' has levels not trained on: [\"A. G. Johnsons vei\", \"Aa\", \"Aarø\", \"Akebakkeskogen\", \"Algarheim\", \"Almås\", \"Alvestad\", \"Amalienborg\", \"Amundrud\", \"Andenes 5\", ...1008 not listed..., \"Øvre Damsgård\", \"Øvre Lillebakken/Diagonalgata\", \"Øvre Mosby\", \"Øvre Skyset\", \"Øvre Svatsum\", \"Øvrebotten\", \"Øye\", \"Øye/Stennes\", \"Øyerne\", \"Øyfjell\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__address' has levels not trained on: [\"17. MAI GATA 6\", \"4. STRØM TERRASSE 13\", \"A B AARSTENS GATE 4\", \"AAGAARDS PLASS 2\", \"AAGAARDVEIEN 177\", \"AARSTADGATEN 6\", \"AASGAARDEN 4\", \"ABEL MEYERS GATE 13\", \"ABELSBORG GATE 10\", \"ADMIRAL BØRRESENS VEI 6 E\", ...4341 not listed..., \"ØYEVOLLVEIEN 6\", \"ØYGARDEN 18\", \"ØYGARDSBAKKEN 13 A\", \"ØYJORDSVEIEN 1\", \"ØYRAGATA 9\", \"ØYRAPLASSEN 9\", \"ØYRO 17\", \"ØYRO 2\", \"ØYRO 39\", \"ØYVIND LAMBES VEI 6\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__lv2' has levels not trained on: [\"1_5\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__lv3' has levels not trained on: [\"1_5_1\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__lv4' has levels not trained on: [\"1_5_1_0\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__firstname' has levels not trained on: [\"0047 OSLO\", \"1001\", \"1001 IMPORT TROMSØ\", \"2 FRISØRER MØRSTAD\", \"2 WEAR AMFI\", \"24-7 TRENIGSSENTER\", \"24-7 TRENINGSSENTER\", \"3 BYGG ØSTFOLD\", \"3ROOMS\", \"3T MELHUS\", ...6588 not listed..., \"ØSTVOLD ERNST\", \"ØVERLANDS ARNULF GALLERI\", \"ØVERÅS TORE\", \"ØVRE SUKKE GÅRD\", \"ØYA FYRVERKERI\", \"ØYE GUNNAR\", \"ØYENE OPTIKK\", \"ØYENLEGE\", \"ØYEREN BÅTSERVICE\", \"ØYNOR CONSULT\"]\n",
      "  warnings.warn(w)\n",
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/job.py:83: UserWarning: Test/Validation dataset column 'remainder__lastname' has levels not trained on: [\"&\", \"100\", \"101\", \"11\", \"16\", \"17\", \"1716\", \"18\", \"1869\", \"1877\", ...2869 not listed..., \"ØRNABERGTUNET\", \"ØRTING\", \"ØSTBANEHALLEN\", \"ØSTERLIE\", \"ØSTERVEIEN\", \"ØSTERØYVEIEN\", \"ØVREBØ\", \"ØYGARDEN\", \"ØYJORDEN\", \"ØYSTEIN\"]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "███████████████████████████████████████████| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "preds_avg = aml.predict(test)\n",
    "preds_best = aml.leader.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/sklearn/base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- predicted\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- revenue\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = test.cbind(preds_best)\n",
    "df = df.as_data_frame(use_pandas=True)\n",
    "result = df.loc[:,(\"remainder__store_id\", 'predict')]\n",
    "submission = result.rename(columns = {\"remainder__store_id\": \"id\",  \"predict\" : \"predicted\"})\n",
    "submission['predicted'] = pt.inverse_transform(submission[['predicted']])\n",
    "submission.to_csv(\"sun_2233_submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sklearn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b486652d2e6c5ac00f1af9aaa5d14fac25fa6ee0068e5fd7a5ee238732fb741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
