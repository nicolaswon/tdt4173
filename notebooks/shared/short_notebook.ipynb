{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_read(path, file_name, dtype=None):\n",
    "    \"\"\"\n",
    "        Utility function to simplify reading of files from local machine.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"{path}{file_name}.csv\", dtype=dtype)\n",
    "\n",
    "def deduplicate_year(raw_df, deduplicate_column=\"grunnkrets_id\"):\n",
    "    \"\"\"\n",
    "        Use 2016 values by default. If exist in 2015, merge together. Drop year.\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.copy()\n",
    "    return raw_df.sort_values(by='year').drop_duplicates(subset=[deduplicate_column], keep='last').drop('year', axis=1)\n",
    "\n",
    "def combine_keys(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe['t_district'] = dataframe['district_name'] + dataframe['municipality_name']\n",
    "    return dataframe\n",
    "\n",
    "def bus_stops_lat_lon(bus_stops_df):\n",
    "    \"\"\"\n",
    "    Extract latitude and longitude as separate columns.\n",
    "    \"\"\"\n",
    "    bus_stops_df['lng_lat'] = bus_stops_df['geometry'].str.extract(\n",
    "        r'\\((.*?)\\)')\n",
    "    bus_stops_df[['lon', 'lat']] = bus_stops_df['lng_lat'].str.split(\n",
    "        \" \", 1, expand=True)\n",
    "    bus_stops_df[['lon', 'lat']] = bus_stops_df[[\n",
    "        'lon', 'lat']].apply(pd.to_numeric)\n",
    "    return bus_stops_df[['busstop_id', 'stopplace_type', 'importance_level', 'side_placement', 'geometry', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population(dataset_age):\n",
    "    \"\"\"\n",
    "        Calculate total population of grunnkrets\n",
    "    \"\"\"\n",
    "    population = dataset_age.drop([\"grunnkrets_id\"], axis=1).sum(axis=1)\n",
    "    dataset_age[\"population_count\"] = population\n",
    "    return dataset_age[[\"grunnkrets_id\", \"population_count\"]]\n",
    "\n",
    "def population_grouped(data_age, data_geography, grouping_element):\n",
    "    \"\"\"\n",
    "        Calculate population of a given geographical grouping.\n",
    "    \"\"\"\n",
    "    age_df = population(data_age)\n",
    "    geography_df = data_geography\n",
    "    population_df = age_df.merge(geography_df, how=\"left\", on=\"grunnkrets_id\")\n",
    "    grouped_df = population_df.groupby([grouping_element], as_index=False)[\"population_count\"].sum()\n",
    "    return grouped_df\n",
    "\n",
    "def population_count_grouped_by_geo_group(stores_df, age_df, grunnkrets_df, geo_groups): \n",
    "    \"\"\"\n",
    "        Calculate population of all geographical groupings.\n",
    "    \"\"\"\n",
    "    combined_df = stores_df.merge(grunnkrets_df, how = \"left\", on = \"grunnkrets_id\")\n",
    "\n",
    "    population_columns = [\"population_count\"]\n",
    "    df_list = []\n",
    "\n",
    "    for geo_group in geo_groups: \n",
    "        pop_df = population_grouped(age_df, grunnkrets_df, geo_group)\n",
    "        merged_df = combined_df.merge(pop_df, how = \"left\", on = geo_group)[[\"store_id\"] + population_columns]\n",
    "        merged_df.set_index(\"store_id\", inplace = True)\n",
    "        merged_df2 = merged_df.add_prefix(f'{geo_group}_')\n",
    "        df_list.append(merged_df2)\n",
    "\n",
    "    return pd.concat(df_list, axis = 1).reset_index()\n",
    "\n",
    "def population_density(age_df, geo_df, grouping_element):\n",
    "    \"\"\"\n",
    "        Calculate population density based on population and area for a given geographical grouping.\n",
    "    \"\"\"\n",
    "    age_data = population(age_df)\n",
    "    geo_df = geo_df\n",
    "    combined_df = age_data.merge(geo_df, how=\"left\", on=\"grunnkrets_id\")\n",
    "    density_df = combined_df.groupby([grouping_element], as_index=False)[\n",
    "        [\"population_count\", \"area_km2\"]].sum()\n",
    "    density_df[\"density\"] = density_df[\"population_count\"] / \\\n",
    "        density_df[\"area_km2\"]\n",
    "    return density_df\n",
    "\n",
    "def population_density_grouped_by_geo_group(stores_df, age_df, grunnkrets_df, geo_groups):\n",
    "    \"\"\"\n",
    "        Calculate population density based on population and area for all geographical grouping.\n",
    "    \"\"\"\n",
    "    grunnkrets_df_2016 = grunnkrets_df\n",
    "    combined_df = stores_df.merge(grunnkrets_df_2016, how = \"left\", on = \"grunnkrets_id\")\n",
    "\n",
    "    pop_density_columns = [\"density\"]\n",
    "    df_list = []\n",
    "\n",
    "    for geo_group in geo_groups: \n",
    "        pop_df = population_density(age_df, grunnkrets_df, geo_group)\n",
    "        merged_df = combined_df.merge(pop_df, how = \"left\", on = geo_group)[[\"store_id\"] + pop_density_columns]\n",
    "        merged_df.set_index(\"store_id\", inplace = True)\n",
    "        merged_df2 = merged_df.add_prefix(f'{geo_group}_')\n",
    "        df_list.append(merged_df2)\n",
    "\n",
    "    return pd.concat(df_list, axis = 1).reset_index()\n",
    "\n",
    "def new_pop_density(stores_df, age_dist, grunnkrets_df, geo_groups):\n",
    "    population_density = population_density_grouped_by_geo_group(stores_df, age_dist, grunnkrets_df, geo_groups)\n",
    "    return population_density.fillna(population_density.mean())\n",
    "\n",
    "def bus_stops_closest(stores_df, bus_stops_df, importance_level=\"Regionalt knutepunkt\"):\n",
    "    \"\"\"\n",
    "    Id and distance of the closest bus stop to all stores.\n",
    "    \"\"\"\n",
    "    bus_stops_df = bus_stops_df[bus_stops_df['importance_level'] == importance_level]\n",
    "    mat = cdist(stores_df[['lat', 'lon']],\n",
    "                bus_stops_df[['lat', 'lon']], metric='euclidean')\n",
    "\n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=bus_stops_df['busstop_id'])\n",
    "\n",
    "    stores = stores_df.store_id\n",
    "    closest = new_df.idxmin(axis=1)\n",
    "    distance = new_df.min(axis=1)\n",
    "\n",
    "    return pd.DataFrame({'store_id': stores.values, 'closest_bus_stop': closest.values, 'distance': distance.values})\n",
    "\n",
    "def bus_stops_in_radius(stores_df, bus_stops_df, radius=0.1, importance_level=None):\n",
    "    \"\"\"\n",
    "    Number of bus stops within a given radius. The importance level of bus stops can be specified.\n",
    "    \"\"\"\n",
    "    if importance_level is not None:\n",
    "        bus_stops_df = bus_stops_df[bus_stops_df['importance_level'] == importance_level]\n",
    "\n",
    "    mat = cdist(stores_df[['lat', 'lon']],\n",
    "                bus_stops_df[['lat', 'lon']], metric='euclidean')\n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=bus_stops_df['busstop_id'])\n",
    "    count = pd.DataFrame(new_df[new_df < radius].count(axis=1)).reset_index()\n",
    "    count.rename(columns={0: 'count'}, inplace=True)\n",
    "    return count\n",
    "\n",
    "# Relevant feature engineering functions.\n",
    "def bus_stops_distance_by_importance(stores_df, bus_stops_df, stop_importance_levels):\n",
    "    \"\"\"\n",
    "    Distance for each store to the closest bus stop of each importance_level\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    for importance_level in stop_importance_levels:\n",
    "        importance_level_cleaned = importance_level.lower().replace(\" \", \"_\")\n",
    "        df = bus_stops_closest(stores_df, bus_stops_df, importance_level=importance_level)\n",
    "        df.rename(columns={'distance': f'distance_to_{importance_level_cleaned}'}, inplace=True)\n",
    "        df_list.append(df[['store_id', f'distance_to_{importance_level_cleaned}']])\n",
    "\n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "def bus_stops_in_radius_by_importance(stores_df, bus_stops_df, stop_importance_levels, radius=0.01):\n",
    "    \"\"\"\n",
    "    Number of bus stops in radius of store for each importance level.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    df_list.append(bus_stops_in_radius(stores_df, bus_stops_df, radius=radius).rename(columns={'count':'number_of_all_stop_types'})) # All bus stops in radius\n",
    "    \n",
    "    for importance_level in stop_importance_levels:\n",
    "        importance_level_cleaned = importance_level.lower().replace(\" \", \"_\")\n",
    "        df = bus_stops_in_radius(stores_df, bus_stops_df, importance_level=importance_level, radius=radius)\n",
    "        df.rename(columns={'count': f'number_of_{importance_level_cleaned}'}, inplace=True)\n",
    "        df_list.append(df[['store_id', f'number_of_{importance_level_cleaned}']])\n",
    "\n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "def store_closest(stores_df, compare_df, store_type_group=\"lv4_desc\"):\n",
    "    \"\"\"\n",
    "    Id and distance of the closest store of same type in the same group.\n",
    "    \"\"\"\n",
    "    \n",
    "    store_types_in_group = stores_df[store_type_group].unique()\n",
    "    df_list = []\n",
    "    for store_type in store_types_in_group:\n",
    "        stores_by_type = stores_df[stores_df[store_type_group] == store_type]\n",
    "        stores_comp_by_type = compare_df[compare_df[store_type_group] == store_type]\n",
    "        \n",
    "        mat = cdist(stores_by_type[['lat', 'lon']], stores_comp_by_type[['lat', 'lon']], metric='euclidean')\n",
    "        \n",
    "        df = pd.DataFrame(\n",
    "            mat, index=stores_by_type['store_id'], columns=stores_comp_by_type['store_id'])\n",
    "        \n",
    "        df = df[df > 0]\n",
    "        \n",
    "        stores = df.index\n",
    "        closest = df.idxmin(axis=1)\n",
    "        distance = df.min(axis=1)\n",
    "        \n",
    "        new_df = pd.DataFrame({'store_id': stores.values, 'closest_store': closest.values, 'distance': distance.values})\n",
    "        df_list.append(new_df)\n",
    "        \n",
    "    \n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "def store_closest_by_store_groups(stores_df, compare_df, store_type_groups):\n",
    "    \"\"\"\n",
    "        Closest store across all store levels.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    \n",
    "    for store_type_group in store_type_groups:\n",
    "        df = store_closest(stores_df, compare_df, store_type_group=store_type_group)\n",
    "        df.rename(columns={'distance': f'distance_to_{store_type_group}'}, inplace=True)\n",
    "        df_list.append(df[['store_id', f'distance_to_{store_type_group}']])\n",
    "\n",
    "    dfs = [df.set_index('store_id') for df in df_list]\n",
    "    return pd.concat(dfs, axis=1).reset_index()\n",
    "\n",
    "def distance_to_closest_group(stores_df, compare_df, group):\n",
    "    \"\"\"\n",
    "        Mall or chain\n",
    "    \"\"\"\n",
    "    mat = cdist(\n",
    "        stores_df[['lat', 'lon']],\n",
    "        compare_df[compare_df[group].notna()][['lat', 'lon']], metric=\"euclidean\"\n",
    "    )\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        mat, index=stores_df['store_id'], columns=compare_df[compare_df[group].notna()]['store_id']\n",
    "    )\n",
    "    \n",
    "    new_df = new_df[new_df > 0]\n",
    "    \n",
    "    stores = new_df.index\n",
    "    # closest = new_df.idxmin(axis=1)\n",
    "    distance = new_df.min(axis=1)\n",
    "    \n",
    "    return pd.DataFrame({'store_id': stores.values, f'distance_closest_{group}': distance.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/318296919.py:25: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  bus_stops_df[['lon', 'lat']] = bus_stops_df['lng_lat'].str.split(\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"/Users/nwong/Workspace/Projects/tdt4173_project/data/raw/\"\n",
    "\n",
    "raw_stores_train = raw_read(raw_path, \"stores_train\", {'grunnkrets_id':str})\n",
    "raw_stores_test = raw_read(raw_path, \"stores_test\", {'grunnkrets_id':str})\n",
    "raw_stores_extra = raw_read(raw_path,\"stores_extra\", {'grunnkrets_id':str})\n",
    "\n",
    "raw_income_dist = raw_read(raw_path, \"grunnkrets_income_households\", {'grunnkrets_id':str})\n",
    "raw_age_dist = raw_read(raw_path, \"grunnkrets_age_distribution\", {'grunnkrets_id':str})\n",
    "raw_households_dist = raw_read(raw_path, \"grunnkrets_households_num_persons\", {'grunnkrets_id':str})\n",
    "raw_grunnkrets = raw_read(raw_path, \"grunnkrets_norway_stripped\", {'grunnkrets_id':str})\n",
    "\n",
    "raw_plaace = raw_read(raw_path, \"plaace_hierarchy\", {'lv1':str, 'lv2':str})\n",
    "raw_bus_stops = raw_read(raw_path, \"busstops_norway\")\n",
    "\n",
    "dedup_income_dist = deduplicate_year(raw_income_dist)\n",
    "dedup_age_dist = deduplicate_year(raw_age_dist)\n",
    "dedup_households_dist = deduplicate_year(raw_households_dist)\n",
    "dedup_grunnkrets = combine_keys(deduplicate_year(raw_grunnkrets))\n",
    "\n",
    "enriched_bus_stops = bus_stops_lat_lon(raw_bus_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train_merged = raw_stores_train.merge(raw_plaace, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "stores_test_merged = raw_stores_test.merge(raw_plaace, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "stores_extra_merged = raw_stores_extra.merge(raw_plaace, on=\"plaace_hierarchy_id\", how=\"left\")\n",
    "\n",
    "compare_train_df = pd.concat([stores_train_merged, stores_extra_merged], ignore_index=True)\n",
    "compare_test_df = pd.concat([stores_test_merged, stores_extra_merged], ignore_index=True)\n",
    "\n",
    "stores_types = ['lv1_desc', 'lv2_desc', 'lv3_desc', 'lv4_desc']\n",
    "geo_groups = ['grunnkrets_id', 't_district', 'municipality_name']\n",
    "stop_importance_levels = ['Mangler viktighetsnivå',\n",
    "                          'Standard holdeplass',\n",
    "                          'Lokalt knutepunkt',\n",
    "                          'Nasjonalt knutepunkt',\n",
    "                          'Regionalt knutepunkt',\n",
    "                          'Annen viktig holdeplass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/531633061.py:71: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return population_density.fillna(population_density.mean())\n",
      "/var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/ipykernel_97639/531633061.py:71: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  return population_density.fillna(population_density.mean())\n"
     ]
    }
   ],
   "source": [
    "merged_stores_train = stores_train_merged \\\n",
    "    .merge(dedup_income_dist, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(store_closest_by_store_groups(stores_train_merged, compare_train_df, stores_types), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_pop_density(raw_stores_train, dedup_age_dist, dedup_grunnkrets, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(bus_stops_distance_by_importance(raw_stores_train, enriched_bus_stops, stop_importance_levels).reset_index(level=0), on=\"store_id\", how=\"left\")\n",
    "    \n",
    "merged_stores_test = stores_test_merged \\\n",
    "    .merge(dedup_income_dist, on=\"grunnkrets_id\", how=\"left\") \\\n",
    "    .merge(store_closest_by_store_groups(stores_test_merged, compare_test_df, stores_types), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(new_pop_density(raw_stores_test, dedup_age_dist, dedup_grunnkrets, geo_groups), on=\"store_id\", how=\"left\") \\\n",
    "    .merge(bus_stops_distance_by_importance(raw_stores_test, enriched_bus_stops, stop_importance_levels).reset_index(level=0), on=\"store_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_stores_train\n",
    "\n",
    "def firstname(string): \n",
    "    if \" \" in string: \n",
    "        newstring = string.split(\" \")[:-1]\n",
    "        return \" \".join(newstring)\n",
    "    else: \n",
    "        return string\n",
    "\n",
    "def lastname(string):\n",
    "    if \" \" in string: \n",
    "        if string.split(\" \")[-1] == 'AS': \n",
    "            return string.split(\" \")[-2]\n",
    "        else: \n",
    "            return string.split(\" \")[-1]  \n",
    "    else: \n",
    "        return string\n",
    "\n",
    "\n",
    "def address(string): \n",
    "    newstring = \"\"\n",
    "    for char in string: \n",
    "        if not char.isdigit(): \n",
    "            newstring += char\n",
    "        else: \n",
    "            return newstring\n",
    "\n",
    "def district(string): \n",
    "    string = string.lower()\n",
    "    if 'sentrum' in string:\n",
    "        return 'sentrum'\n",
    "    else: \n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = merged_stores_train[['store_id', 'revenue']].copy()\n",
    "merged_stores_train = merged_stores_train.copy().drop('revenue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_cols = [\n",
    "    'store_id',\n",
    "    'store_name',\n",
    "    'sales_channel_name_x',\n",
    "    'plaace_hierarchy_id',\n",
    "    'grunnkrets_id',\n",
    "    'address',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'chain_name',\n",
    "    'mall_name',\n",
    "    \n",
    "    'lv1_desc',\n",
    "    'lv2_desc',\n",
    "    'lv3_desc',\n",
    "    'lv4_desc',\n",
    "]\n",
    "\n",
    "yeo_cols = [\n",
    "    'all_households',\n",
    "    'singles',\n",
    "    'couple_without_children',\n",
    "    'couple_with_children',\n",
    "    'other_households',\n",
    "    'single_parent_with_children',\n",
    "]\n",
    "\n",
    "box_cols = [\n",
    "    'distance_to_lv1_desc',\n",
    "    'distance_to_lv2_desc',\n",
    "    'distance_to_lv3_desc',\n",
    "    'distance_to_lv4_desc',\n",
    "    \n",
    "    'grunnkrets_id_density',\n",
    "    't_district_density',\n",
    "    'municipality_name_density',\n",
    "    \n",
    "    'distance_to_lokalt_knutepunkt',\n",
    "    'distance_to_regionalt_knutepunkt',\n",
    "    'distance_to_annen_viktig_holdeplass',\n",
    "    'distance_to_nasjonalt_knutepunkt',\n",
    "    'distance_to_mangler_viktighetsnivå',\n",
    "    'distance_to_standard_holdeplass',\n",
    "]\n",
    "\n",
    "_merged_stores_train = merged_stores_train.filter(inc_cols+yeo_cols+box_cols)\n",
    "_merged_stores_test = merged_stores_test.filter(inc_cols+yeo_cols+box_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PowerTransformer()\n",
    ")\n",
    "box_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    PowerTransformer(method=\"box-cox\")\n",
    ")\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "        (yeo_pipeline, yeo_cols),\n",
    "        (box_pipeline, box_cols),\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "def new_transformer(merged_stores_df, preprocessing):\n",
    "    return pd.DataFrame(preprocessing.fit_transform(merged_stores_df), columns=preprocessing.get_feature_names_out(), index=merged_stores_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_stores_train = new_transformer(_merged_stores_train, preprocessing)\n",
    "_merged_stores_test = new_transformer(_merged_stores_test, preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer()\n",
    "rev_transformed = pt.fit_transform(target_labels[[\"revenue\"]])\n",
    "_merged_stores_train[\"revenue\"] = rev_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merged_stores_train = _merged_stores_train[(_merged_stores_train.revenue > -1.8888)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_301\"; Java(TM) SE Runtime Environment (build 1.8.0_301-b09); Java HotSpot(TM) 64-Bit Server VM (build 25.301-b09, mixed mode)\n",
      "  Starting server from /Users/nwong/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/tmp31pontms\n",
      "  JVM stdout: /var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/tmp31pontms/h2o_nwong_started_from_python.out\n",
      "  JVM stderr: /var/folders/z1/l03w8mpn5xz3mghrk0j2w5gr0000gn/T/tmp31pontms/h2o_nwong_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Oslo</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.38.0.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>17 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_nwong_ekvwsk</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.549 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.4 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Europe/Oslo\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.38.0.2\n",
       "H2O_cluster_version_age:    17 days\n",
       "H2O_cluster_name:           H2O_from_python_nwong_ekvwsk\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.549 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.4 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "ename": "H2OResponseError",
     "evalue": "Server error java.lang.IllegalArgumentException:\n  Error: Column remainder__sales_channel_name not found\n  Request: POST /99/Rapids\n    data: {'ast': \"(tmp= py_2_sid_9e47 (cols_py (tmp= py_1_sid_9e47 (:= Key_Frame__upload_a7d34d808fb027f7707a788862e74773.hex (as.factor (cols_py Key_Frame__upload_a7d34d808fb027f7707a788862e74773.hex 'remainder__store_name')) 20 [])) 'remainder__sales_channel_name'))\", 'session_id': '_sid_9e47'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OResponseError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/Users/nwong/Workspace/Projects/tdt4173_project/notebooks/shared/short_notebook.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nwong/Workspace/Projects/tdt4173_project/notebooks/shared/short_notebook.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cat_vars \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mremainder__\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m cat_vars \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstore_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nwong/Workspace/Projects/tdt4173_project/notebooks/shared/short_notebook.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m cat \u001b[39min\u001b[39;00m cat_vars:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nwong/Workspace/Projects/tdt4173_project/notebooks/shared/short_notebook.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     train[cat] \u001b[39m=\u001b[39m train[cat]\u001b[39m.\u001b[39;49masfactor()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nwong/Workspace/Projects/tdt4173_project/notebooks/shared/short_notebook.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     test[cat] \u001b[39m=\u001b[39m test[cat]\u001b[39m.\u001b[39masfactor()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/frame.py:3273\u001b[0m, in \u001b[0;36mH2OFrame.asfactor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[39mConvert column/columns in the current frame to categoricals.\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3270\u001b[0m \u001b[39m>>> df[['cylinders','economy_20mpg']].describe()\u001b[39;00m\n\u001b[1;32m   3271\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3272\u001b[0m \u001b[39mfor\u001b[39;00m colname \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames:\n\u001b[0;32m-> 3273\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtypes[colname]\n\u001b[1;32m   3274\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mbool\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39menum\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m   3275\u001b[0m         \u001b[39mraise\u001b[39;00m H2OValueError(\u001b[39m\"\u001b[39m\u001b[39mOnly \u001b[39m\u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are allowed for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3276\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39masfactor(), got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (colname, t))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/frame.py:366\u001b[0m, in \u001b[0;36mH2OFrame.types\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ex\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mtypes_valid():\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ex\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mflush()\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_frame(fill_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    367\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ex\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mtypes)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/frame.py:572\u001b[0m, in \u001b[0;36mH2OFrame._frame\u001b[0;34m(self, rows, rows_offset, cols, cols_offset, fill_cache)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_frame\u001b[39m(\u001b[39mself\u001b[39m, rows\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, rows_offset\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, cols\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cols_offset\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, fill_cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ex\u001b[39m.\u001b[39;49m_eager_frame()\n\u001b[1;32m    573\u001b[0m     \u001b[39mif\u001b[39;00m fill_cache:\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ex\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mfill(rows\u001b[39m=\u001b[39mrows, rows_offset\u001b[39m=\u001b[39mrows_offset, cols\u001b[39m=\u001b[39mcols, cols_offset\u001b[39m=\u001b[39mcols_offset)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/expr.py:91\u001b[0m, in \u001b[0;36mExprNode._eager_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mis_empty(): \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39m_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m  \u001b[39m# Data already computed under ID, but not cached locally\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_eval_driver(\u001b[39m'\u001b[39;49m\u001b[39mframe\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/expr.py:115\u001b[0m, in \u001b[0;36mExprNode._eval_driver\u001b[0;34m(self, top)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m:param top: if this is a top expression (providing a final result),\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m    then specifies the expected result type (accepted values = ['frame', 'scalar']),\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    or None if no object creation is expected.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m:return: self expr\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m exec_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ast_str(top)\n\u001b[0;32m--> 115\u001b[0m res \u001b[39m=\u001b[39m ExprNode\u001b[39m.\u001b[39;49mrapids(exec_str)\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mscalar\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m res:\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res[\u001b[39m'\u001b[39m\u001b[39mscalar\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mlist\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/expr.py:259\u001b[0m, in \u001b[0;36mExprNode.rapids\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrapids\u001b[39m(expr):\n\u001b[1;32m    252\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m    Execute a Rapids expression.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m    :returns: The JSON response (as a python dictionary) of the Rapids execution\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mreturn\u001b[39;00m h2o\u001b[39m.\u001b[39;49mapi(\u001b[39m\"\u001b[39;49m\u001b[39mPOST /99/Rapids\u001b[39;49m\u001b[39m\"\u001b[39;49m, data\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mast\u001b[39;49m\u001b[39m\"\u001b[39;49m: expr, \u001b[39m\"\u001b[39;49m\u001b[39msession_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: h2o\u001b[39m.\u001b[39;49mconnection()\u001b[39m.\u001b[39;49msession_id})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/h2o.py:124\u001b[0m, in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m# type checks are performed in H2OConnection class\u001b[39;00m\n\u001b[1;32m    123\u001b[0m _check_connection()\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m h2oconn\u001b[39m.\u001b[39;49mrequest(endpoint, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, filename\u001b[39m=\u001b[39;49mfilename, save_to\u001b[39m=\u001b[39;49msave_to)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/backend/connection.py:498\u001b[0m, in \u001b[0;36mH2OConnection.request\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    496\u001b[0m         save_to \u001b[39m=\u001b[39m save_to(resp)\n\u001b[1;32m    497\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_end_transaction(start_time, resp)\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_response(resp, save_to)\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mHTTPError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_server \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_server\u001b[39m.\u001b[39mis_running():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sklearn-env/lib/python3.10/site-packages/h2o/backend/connection.py:852\u001b[0m, in \u001b[0;36mH2OConnection._process_response\u001b[0;34m(response, save_to)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m status_code \u001b[39min\u001b[39;00m {\u001b[39m400\u001b[39m, \u001b[39m404\u001b[39m, \u001b[39m412\u001b[39m} \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, H2OErrorV3):\n\u001b[1;32m    851\u001b[0m     data\u001b[39m.\u001b[39mshow_stacktrace \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     \u001b[39mraise\u001b[39;00m H2OResponseError(data)\n\u001b[1;32m    854\u001b[0m \u001b[39m# Server errors (notably 500 = \"Server Error\")\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# Note that it is possible to receive valid H2OErrorV3 object in this case, however it merely means the server\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# did not provide the correct status code.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mraise\u001b[39;00m H2OServerError(\u001b[39m\"\u001b[39m\u001b[39mHTTP \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (status_code, response\u001b[39m.\u001b[39mreason, data))\n",
      "\u001b[0;31mH2OResponseError\u001b[0m: Server error java.lang.IllegalArgumentException:\n  Error: Column remainder__sales_channel_name not found\n  Request: POST /99/Rapids\n    data: {'ast': \"(tmp= py_2_sid_9e47 (cols_py (tmp= py_1_sid_9e47 (:= Key_Frame__upload_a7d34d808fb027f7707a788862e74773.hex (as.factor (cols_py Key_Frame__upload_a7d34d808fb027f7707a788862e74773.hex 'remainder__store_name')) 20 [])) 'remainder__sales_channel_name'))\", 'session_id': '_sid_9e47'}\n"
     ]
    }
   ],
   "source": [
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "train = h2o.H2OFrame(_merged_stores_train)\n",
    "test = h2o.H2OFrame(_merged_stores_test)\n",
    "\n",
    "cat_vars = inc_cols\n",
    "\n",
    "cat_vars = [f'remainder__{i}' for i in cat_vars if i != 'store_id']\n",
    "\n",
    "for cat in cat_vars:\n",
    "    train[cat] = train[cat].asfactor()\n",
    "    test[cat] = test[cat].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c34ca09daac3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"revenue\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run AutoML for 20 base models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "x = train.columns\n",
    "y = \"revenue\"\n",
    "x.remove(y)\n",
    "\n",
    "# Run AutoML for 20 base models\n",
    "aml = H2OAutoML(max_models=20, seed=1, exclude_algos=['deeplearning'])\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a01f58376ff7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpreds_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aml' is not defined"
     ]
    }
   ],
   "source": [
    "preds_avg = aml.predict(test)\n",
    "preds_best = aml.leader.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test.cbind(preds_best)\n",
    "df = df.as_data_frame(use_pandas=True)\n",
    "result = df.loc[:,(\"remainder__store_id\", 'predict')]\n",
    "submission = result.rename(columns = {\"remainder__store_id\": \"id\",  \"predict\" : \"predicted\"})\n",
    "submission['predicted'] = pt.inverse_transform(submission[['predicted']])\n",
    "submission.to_csv(\"StackedEnsembleBestOfFamily21.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sklearn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b486652d2e6c5ac00f1af9aaa5d14fac25fa6ee0068e5fd7a5ee238732fb741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
